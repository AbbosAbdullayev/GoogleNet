{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abbos\\.conda\\envs\\lanedet\\lib\\site-packages\\tqdm-4.64.0-py3.8.egg\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "## device\n",
    "#device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self,input_ch,out1x1,red3x3,out3x3,red5x5,out5x5,pool_proj):\n",
    "        super(Inception,self).__init__()\n",
    "        self.arm1x1=nn.Sequential(\n",
    "            nn.Conv2d(input_ch,out1x1,kernel_size=1),\n",
    "            nn.BatchNorm2d(out1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.arm3x3=nn.Sequential(\n",
    "            nn.Conv2d(input_ch,red3x3,kernel_size=1),\n",
    "            nn.BatchNorm2d(red3x3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(red3x3,out3x3,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(out3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "       \n",
    "        self.arm5x5=nn.Sequential(\n",
    "            nn.Conv2d(input_ch,red5x5,kernel_size=1),\n",
    "            nn.BatchNorm2d(red5x5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(red5x5,out5x5,5,padding=2),\n",
    "            nn.BatchNorm2d(out5x5),\n",
    "            nn.ReLU(True),\n",
    "        )       \n",
    " \n",
    "        self.arm_pool=nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3,stride=1,padding=1),\n",
    "            nn.Conv2d(input_ch,pool_proj,kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_proj),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self,m):\n",
    "        arm1=self.arm1x1(m)\n",
    "        arm3=self.arm3x3(m)\n",
    "        arm5=self.arm5x5(m)\n",
    "        pool_proj=self.arm_pool(m)\n",
    "        return torch.cat([arm1,arm3,arm5,pool_proj],1)\n",
    "\n",
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self,n_class):\n",
    "        super(GoogleNet,self).__init__()\n",
    "        self.conv_block=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=7,stride=2,padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n",
    "            nn.Conv2d(in_channels=64,out_channels=192,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n",
    "        )\n",
    "        self.inception3a=Inception(192,64,96,128,16,32,32)\n",
    "        self.inception3b=Inception(256,128,128,192,32,96,64)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        self.inception4a=Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b=Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c=Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d=Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e=Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5a=Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b=Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool=nn.AvgPool2d(kernel_size=7,stride=1)\n",
    "        self.dropout=nn.Dropout(0.4)\n",
    "        self.fc=nn.Linear(1024,n_class)\n",
    "\n",
    "    def forward(self,m):\n",
    "        m=self.conv_block(m)\n",
    "        m=self.inception3a(m)\n",
    "        m=self.inception3b(m)\n",
    "        m=self.maxpool(m)\n",
    "        m=self.inception4a(m)\n",
    "        m=self.inception4b(m)\n",
    "        m=self.inception4c(m)\n",
    "        m=self.inception4d(m)\n",
    "        m=self.inception4e(m)\n",
    "        m=self.maxpool(m)\n",
    "        m=self.inception5a(m)\n",
    "        m=self.inception5b(m)\n",
    "        m=self.avgpool(m)\n",
    "        m=m.view(m.size(0),-1)\n",
    "        m=self.dropout(m)\n",
    "        m=self.fc(m)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device= device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# the location of image data(main dir)\n",
    "dataset='dataset'  \n",
    "# Custom dataset class\n",
    "class WeatherDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,transform=None):\n",
    "        super(WeatherDataset,self).__init__()\n",
    "        self.dataset=dataset\n",
    "        self.transform=transform\n",
    "\n",
    "        self.images=[]\n",
    "        self.labels=[]\n",
    "\n",
    "        for class_name in os.listdir(dataset):\n",
    "            class_dir=os.path.join(dataset,class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                if class_name=='cloudy':\n",
    "                 label=0\n",
    "            if class_name=='sunrise':\n",
    "                label=1\n",
    "            if class_name=='rainy':\n",
    "                 label=2\n",
    "            if class_name=='shine':\n",
    "                label=3  \n",
    "            for image in os.listdir(class_dir):\n",
    "                image_path=os.path.join(class_dir,image)\n",
    "                self.images.append(image_path)\n",
    "                self.labels.append(label)    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self,idx):\n",
    "        image=torchvision.datasets.folder.pil_loader(self.images[idx])\n",
    "        if self.transform:\n",
    "           image=self.transform(image)\n",
    "        return image, self.labels[idx] \n",
    "transform=transforms.Compose([\n",
    "   transforms.RandomRotation(10),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "data_set=WeatherDataset(dataset,transform=transform)\n",
    "trainset,validset=torch.utils.data.random_split(data_set,[1000,125])\n",
    "train_loader=torch.utils.data.DataLoader(trainset,batch_size=16,shuffle=True)\n",
    "valid_loader=torch.utils.data.DataLoader(validset,batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set=WeatherDataset(dataset,transform=transform)\n",
    "#print(data_set.__len__())\n",
    "#print(data_set.__getitem__(4))\n",
    "samples,labels=iter(train_loader).next()\n",
    "classes={0:'cloudy',1:'sunrise',2:'rainy',3:'shine'}\n",
    "fig=plt.figure(figsize=(8,8))\n",
    "for i in range(16):\n",
    "    a=fig.add_subplot(2,2,i+1)\n",
    "    a.set_title(classes[labels[i].item()])\n",
    "    a.axis('off')\n",
    "    a.imshow(np.transpose(samples[i].numpy(),(1,2,0)))\n",
    "plt.subplots_adjust(bottom=0.2, top=0.6, hspace=0)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.772sec : [Epoch 1/100] train loss: 53.326 -> training accuracy: 43.3000\n",
      "18.199sec : [Epoch 2/100] train loss: 40.597 -> training accuracy: 45.7000\n",
      "17.487sec : [Epoch 3/100] train loss: 35.850 -> training accuracy: 47.3333\n",
      "17.300sec : [Epoch 4/100] train loss: 29.491 -> training accuracy: 48.4750\n",
      "17.298sec : [Epoch 5/100] train loss: 31.985 -> training accuracy: 49.1800\n",
      "17.326sec : [Epoch 6/100] train loss: 22.729 -> training accuracy: 49.9333\n",
      "17.472sec : [Epoch 7/100] train loss: 21.020 -> training accuracy: 50.4429\n",
      "17.666sec : [Epoch 8/100] train loss: 22.116 -> training accuracy: 50.8125\n",
      "17.349sec : [Epoch 9/100] train loss: 18.251 -> training accuracy: 51.2111\n",
      "18.013sec : [Epoch 10/100] train loss: 16.315 -> training accuracy: 51.5400\n",
      "17.325sec : [Epoch 11/100] train loss: 16.298 -> training accuracy: 51.7818\n",
      "17.201sec : [Epoch 12/100] train loss: 19.099 -> training accuracy: 51.9917\n",
      "17.262sec : [Epoch 13/100] train loss: 17.452 -> training accuracy: 52.1308\n",
      "17.264sec : [Epoch 14/100] train loss: 12.922 -> training accuracy: 52.4143\n",
      "17.560sec : [Epoch 15/100] train loss: 12.682 -> training accuracy: 52.6333\n",
      "17.440sec : [Epoch 16/100] train loss: 11.155 -> training accuracy: 52.8125\n",
      "18.217sec : [Epoch 17/100] train loss: 11.659 -> training accuracy: 53.0059\n",
      "18.515sec : [Epoch 18/100] train loss: 13.786 -> training accuracy: 53.1333\n",
      "18.428sec : [Epoch 19/100] train loss: 11.375 -> training accuracy: 53.2842\n",
      "18.435sec : [Epoch 20/100] train loss: 10.177 -> training accuracy: 53.4350\n",
      "18.284sec : [Epoch 21/100] train loss: 10.487 -> training accuracy: 53.5524\n",
      "18.555sec : [Epoch 22/100] train loss: 10.555 -> training accuracy: 53.6545\n",
      "18.615sec : [Epoch 23/100] train loss: 12.542 -> training accuracy: 53.7696\n",
      "18.499sec : [Epoch 24/100] train loss: 12.388 -> training accuracy: 53.8667\n",
      "18.221sec : [Epoch 25/100] train loss: 9.737 -> training accuracy: 53.9520\n",
      "18.550sec : [Epoch 26/100] train loss: 12.588 -> training accuracy: 54.0269\n",
      "18.713sec : [Epoch 27/100] train loss: 9.107 -> training accuracy: 54.1333\n",
      "18.210sec : [Epoch 28/100] train loss: 11.490 -> training accuracy: 54.2107\n",
      "18.349sec : [Epoch 29/100] train loss: 9.792 -> training accuracy: 54.2724\n",
      "18.225sec : [Epoch 30/100] train loss: 7.999 -> training accuracy: 54.3667\n",
      "18.075sec : [Epoch 31/100] train loss: 8.162 -> training accuracy: 54.4290\n",
      "18.507sec : [Epoch 32/100] train loss: 6.537 -> training accuracy: 54.5312\n",
      "18.299sec : [Epoch 33/100] train loss: 7.828 -> training accuracy: 54.6030\n",
      "18.188sec : [Epoch 34/100] train loss: 8.729 -> training accuracy: 54.6735\n",
      "18.431sec : [Epoch 35/100] train loss: 7.162 -> training accuracy: 54.7400\n",
      "18.763sec : [Epoch 36/100] train loss: 17.572 -> training accuracy: 54.7778\n",
      "18.509sec : [Epoch 37/100] train loss: 13.378 -> training accuracy: 54.8027\n",
      "19.394sec : [Epoch 38/100] train loss: 7.499 -> training accuracy: 54.8605\n",
      "19.650sec : [Epoch 39/100] train loss: 9.409 -> training accuracy: 54.9282\n",
      "18.118sec : [Epoch 40/100] train loss: 7.482 -> training accuracy: 54.9800\n",
      "17.937sec : [Epoch 41/100] train loss: 6.087 -> training accuracy: 55.0366\n",
      "17.896sec : [Epoch 42/100] train loss: 3.320 -> training accuracy: 55.1071\n",
      "18.166sec : [Epoch 43/100] train loss: 10.064 -> training accuracy: 55.1488\n",
      "18.055sec : [Epoch 44/100] train loss: 15.682 -> training accuracy: 55.1864\n",
      "18.120sec : [Epoch 45/100] train loss: 7.009 -> training accuracy: 55.2333\n",
      "17.977sec : [Epoch 46/100] train loss: 10.168 -> training accuracy: 55.2717\n",
      "18.355sec : [Epoch 47/100] train loss: 6.490 -> training accuracy: 55.3064\n",
      "18.372sec : [Epoch 48/100] train loss: 4.193 -> training accuracy: 55.3521\n",
      "18.138sec : [Epoch 49/100] train loss: 6.226 -> training accuracy: 55.3898\n",
      "18.100sec : [Epoch 50/100] train loss: 5.691 -> training accuracy: 55.4240\n",
      "16.658sec : [Epoch 51/100] train loss: 4.399 -> training accuracy: 55.4627\n",
      "16.584sec : [Epoch 52/100] train loss: 6.241 -> training accuracy: 55.4885\n",
      "16.467sec : [Epoch 53/100] train loss: 4.340 -> training accuracy: 55.5340\n",
      "16.403sec : [Epoch 54/100] train loss: 4.243 -> training accuracy: 55.5759\n",
      "16.391sec : [Epoch 55/100] train loss: 7.531 -> training accuracy: 55.6018\n",
      "16.686sec : [Epoch 56/100] train loss: 7.719 -> training accuracy: 55.6304\n",
      "16.724sec : [Epoch 57/100] train loss: 4.755 -> training accuracy: 55.6649\n",
      "16.552sec : [Epoch 58/100] train loss: 4.190 -> training accuracy: 55.7034\n",
      "16.626sec : [Epoch 59/100] train loss: 3.426 -> training accuracy: 55.7424\n",
      "16.475sec : [Epoch 60/100] train loss: 5.158 -> training accuracy: 55.7750\n",
      "16.617sec : [Epoch 61/100] train loss: 3.838 -> training accuracy: 55.8131\n",
      "16.536sec : [Epoch 62/100] train loss: 6.361 -> training accuracy: 55.8403\n",
      "16.868sec : [Epoch 63/100] train loss: 4.958 -> training accuracy: 55.8651\n",
      "16.470sec : [Epoch 64/100] train loss: 4.422 -> training accuracy: 55.8906\n",
      "16.805sec : [Epoch 65/100] train loss: 2.749 -> training accuracy: 55.9277\n",
      "16.682sec : [Epoch 66/100] train loss: 8.899 -> training accuracy: 55.9530\n",
      "16.627sec : [Epoch 67/100] train loss: 8.541 -> training accuracy: 55.9567\n",
      "16.740sec : [Epoch 68/100] train loss: 5.751 -> training accuracy: 55.9809\n",
      "16.511sec : [Epoch 69/100] train loss: 4.766 -> training accuracy: 55.9971\n",
      "16.648sec : [Epoch 70/100] train loss: 5.573 -> training accuracy: 56.0271\n",
      "16.708sec : [Epoch 71/100] train loss: 3.510 -> training accuracy: 56.0493\n",
      "16.564sec : [Epoch 72/100] train loss: 2.301 -> training accuracy: 56.0750\n",
      "16.839sec : [Epoch 73/100] train loss: 4.512 -> training accuracy: 56.1014\n",
      "16.747sec : [Epoch 74/100] train loss: 4.853 -> training accuracy: 56.1189\n",
      "16.663sec : [Epoch 75/100] train loss: 3.357 -> training accuracy: 56.1427\n",
      "16.459sec : [Epoch 76/100] train loss: 2.132 -> training accuracy: 56.1697\n",
      "16.607sec : [Epoch 77/100] train loss: 7.217 -> training accuracy: 56.1922\n",
      "16.479sec : [Epoch 78/100] train loss: 5.110 -> training accuracy: 56.2115\n",
      "16.762sec : [Epoch 79/100] train loss: 7.802 -> training accuracy: 56.2152\n",
      "16.516sec : [Epoch 80/100] train loss: 2.424 -> training accuracy: 56.2388\n",
      "16.577sec : [Epoch 81/100] train loss: 0.912 -> training accuracy: 56.2679\n",
      "16.583sec : [Epoch 82/100] train loss: 2.922 -> training accuracy: 56.2866\n",
      "16.671sec : [Epoch 83/100] train loss: 1.838 -> training accuracy: 56.3133\n",
      "16.809sec : [Epoch 84/100] train loss: 2.910 -> training accuracy: 56.3381\n",
      "16.662sec : [Epoch 85/100] train loss: 6.455 -> training accuracy: 56.3482\n",
      "16.649sec : [Epoch 86/100] train loss: 2.759 -> training accuracy: 56.3663\n",
      "16.461sec : [Epoch 87/100] train loss: 1.666 -> training accuracy: 56.3920\n",
      "16.750sec : [Epoch 88/100] train loss: 1.222 -> training accuracy: 56.4125\n",
      "16.804sec : [Epoch 89/100] train loss: 3.608 -> training accuracy: 56.4281\n",
      "16.776sec : [Epoch 90/100] train loss: 2.674 -> training accuracy: 56.4489\n",
      "16.702sec : [Epoch 91/100] train loss: 2.825 -> training accuracy: 56.4593\n",
      "16.723sec : [Epoch 92/100] train loss: 4.078 -> training accuracy: 56.4739\n",
      "16.599sec : [Epoch 93/100] train loss: 4.324 -> training accuracy: 56.4892\n",
      "16.679sec : [Epoch 94/100] train loss: 3.503 -> training accuracy: 56.5053\n",
      "16.700sec : [Epoch 95/100] train loss: 3.523 -> training accuracy: 56.5179\n",
      "16.805sec : [Epoch 96/100] train loss: 1.811 -> training accuracy: 56.5375\n",
      "16.641sec : [Epoch 97/100] train loss: 1.347 -> training accuracy: 56.5577\n",
      "16.670sec : [Epoch 98/100] train loss: 1.869 -> training accuracy: 56.5796\n",
      "16.480sec : [Epoch 99/100] train loss: 1.309 -> training accuracy: 56.5970\n",
      "16.919sec : [Epoch 100/100] train loss: 2.421 -> training accuracy: 56.6120\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from nn import *\n",
    "\n",
    "device= device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=GoogleNet(n_class=4).to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimzer=optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "train_loss,val_loss,train_cor=0,0,0\n",
    "total,total_val=0,0\n",
    "train_acc=[]\n",
    "train_losses=[]\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "   train_loss=0.0\n",
    "   start=time.time()\n",
    "   for i,data in enumerate(train_loader,0):\n",
    "        inputs,labels=data\n",
    "        inputs,labels=inputs.to(device),labels.to(device)\n",
    "        optimzer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        train_loss+=loss.item()\n",
    "        _,y_pred=torch.max(outputs,1)    \n",
    "        y_pred[y_pred>=0.5]=1\n",
    "        y_pred[y_pred<=0.5]=0\n",
    "        train_cor+=(y_pred==labels).sum().item()\n",
    "        total+=labels.size(0)\n",
    "        train_losses=train_loss/len(train_loader)\n",
    "        train_acc=100*train_cor/total\n",
    "        ## validation checking\n",
    "        val_loss=0\n",
    "        val_acc=0\n",
    "        valid_correct = 0\n",
    "        '''\n",
    "        for valid_data in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_x,val_y=valid_data\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                pred = model(val_x)\n",
    "                loss = criterion(pred, val_y)\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "                _,y_pred = torch.max(pred,1)\n",
    "                y_pred[y_pred >= 0.5] = 1\n",
    "                y_pred[y_pred < 0.5] = 0\n",
    "                valid_correct +=(y_pred==val_y).sum().item()\n",
    "                total_val+=val_y.size(0)\n",
    "                valid_losses=val_loss/len(valid_loader)   \n",
    "        #train_acc = train_correct/len(train_loader.dataset)\n",
    "                valid_acc = 100*valid_correct/total_val\n",
    "        '''\n",
    "   print(f'{time.time()-start:.3f}sec : [Epoch {epoch+1}/{epochs}] train loss: {train_loss:.3f} -> training accuracy: {train_acc:.4f}') # -> validation loss: {val_loss:.3f} -> validation accuracy: {valid_acc:.3f}')\n",
    "\n",
    "print('Finished training')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lanedet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010ca76ff7b4eb673815e578cd859170cafe8c99e28a8c6e897a6cd4d7b4027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
